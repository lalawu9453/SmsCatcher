# **ü§ù Contributing Guide**

**English** ‚Ä¢ [**ÁπÅÈ´î‰∏≠Êñá**](./CONTRIBUTING.zh-TW.md)

---

Thank you for your interest in the Temporary SMS Receiver Monitor project! Your contributions are vital to enhancing the stability and expanding the features of this scraper.

## **Getting Started**

### **1. Setup Environment**

Please ensure you have installed all dependencies:

```bash
uv sync
```

### **2. Reporting Bugs**

If you encounter any bugs, such as scraping failures, parsing errors due to website structure changes, or SMS content remaining encrypted, please report them via the [Issue tracker](https://github.com/LayorX/Temporary-SMS-Receiver-Monitor/issues).

### **3. Submitting Changes**

We primarily accept Pull Requests (PRs) for new features, bug fixes, and scraper stability improvements.

*   **Fork** the repository first.
*   Create a new branch for your changes.
*   Ensure your code follows PEP 8 and run tests before committing.
*   Clearly describe your changes and what issue they resolve in your PR.

---

## **üß™ Running Tests**

This project includes an automated test suite to ensure code quality and functional stability. Before submitting your changes, please be sure to run the tests locally.

### **1. Install Testing Dependencies**

Testing tools are not included in the main `uv sync`. Please install them separately using the following command:

```bash
uv pip install pytest pytest-mock
```

### **2. Run the Test Suite**

Use the `pytest` module to run all tests. In a Windows environment, to avoid path issues and ensure you are using the correct Python version from your virtual environment, it is recommended to use the full command:

```bash
# Run from the project root directory
.venv\Scripts\python.exe -m pytest -v
```

*   **Unit Tests**: Located in `tests/test_scraper_core.py`, these primarily verify the logic of helper functions like `is_within_last_hour`.
*   **Smoke Tests**: Tests like `test_tempnumber_scraper_smoke_test` execute the main scraper functions to verify they complete successfully and return the correct data type (e.g., `list`), without checking the specific content. This helps to quickly catch fatal execution errors after refactoring.

If you see all test items marked as `PASSED` with no `FAILED` or `ERRORS`, all tests have passed successfully.

### **3. Test Application Startup Parameters**

To test the command-line startup parameters of `main.py`, run the main script with the `--help` argument to see all options:

```bash
uv run python main.py --help
```

---

## **‚úçÔ∏è Framework for Adding New Scrapers**

Want to integrate a new SMS website? Follow these steps to get it done quickly and in a standardized way.

### **Step 1: Initial Site Analysis**

1.  **Open the Target Site**: Open the website you want to scrape in your browser.
2.  **Use Developer Tools**: Press `F12` to open developer tools and select the "Elements" tab.
3.  **Observe Site Behavior**:
    *   **Content Loading**: Is the number list loaded statically (visible in the source code) or dynamically (generated by JavaScript)? This project uses `Selenium` by default, so both can be handled.
    *   **Human Verification**: Is the site protected by Cloudflare or Google reCAPTCHA? This increases scraping difficulty, but `Selenium`'s long wait times can help bypass some verifications.
    *   **Ad Interference**: Does the site have many pop-up or overlay ads? Our `create_adblocking_options` function can effectively handle this.

### **Step 2: Finding Key CSS Selectors**

This is the most critical step. You need to find a stable set of CSS selectors for the new site. Use the developer tools' "Inspect" feature (the small arrow icon in the top-left) to click on page elements and find them in the "Elements" panel.

**You need to record the following selectors:**

1.  **On the Number List Page:**
    *   `number_links_selector`: Points to **each** container element that holds a number link (e.g., `a.country-link`).
    *   `phone_number_text_selector`: Within the container above, points to the element displaying the phone number text (e.g., `h4`).

2.  **On a Single Number's Message Page:**
    *   `message_rows_selector`: Points to **each** message's container element (e.g., `div.direct-chat-msg`).
    *   `time_selector`: Within the message container, points to the element displaying the timestamp (e.g., `time.direct-chat-timestamp`).
    *   `content_selector`: Within the message container, points to the element displaying the SMS content (e.g., `div.direct-chat-text`).

### **Step 3: Writing the Scraper Code**

In `scraper_core.py`, you need to create two functions:

1.  **`[sitename]_find_active_numbers(...)`**: To scrape all numbers from the number list page and check them concurrently.
2.  **`[sitename]_check_single_number(...)`**: To check a single number's page, determine if it's active, and scrape the SMS.

**Code Framework Template:**

```python
# Function 1: Find Active Numbers
def [sitename]_find_active_numbers(CHROME_SERVICE, base_url, user_agent):
    print(f"[*] Searching for numbers on {base_url} using Selenium...")
    numbers_to_check = []
    driver = None
    try:
        # 1. Always use the adblocker
        options = create_adblocking_options(user_agent)
        driver = webdriver.Chrome(service=CHROME_SERVICE, options=options)
        driver.get(base_url) # Or a specific country page URL

        # 2. Wait for the number list to load
        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, "[number_links_selector]")))
        time.sleep(3)

        # 3. Parse the page with BeautifulSoup
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        number_links = soup.select("[number_links_selector]")

        # 4. Iterate through links to create a list of numbers to check
        for link in number_links:
            # ... Parse the number and URL based on your selectors ...
            numbers_to_check.append({'number': phone_number_text, 'url': number_url})

    finally:
        if driver:
            driver.quit()

    # 5. Use a thread pool for concurrent checking
    raw_active_numbers = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Pass in [sitename]_check_single_number
        future_to_number = {executor.submit([sitename]_check_single_number, num_info, user_agent, CHROME_SERVICE, base_url): num_info for num_info in numbers_to_check}
        for future in as_completed(future_to_number):
            result = future.result()
            if result:
                raw_active_numbers.append(result)
    return raw_active_numbers

# Function 2: Check a Single Number
def [sitename]_check_single_number(number_info, user_agent, service, base_url):
    # ... (omitting try/except/finally structure) ...
    driver = webdriver.Chrome(service=service, options=create_adblocking_options(user_agent))
    driver.get(number_info['url'])

    # 1. Wait for the message list to load
    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, "[message_rows_selector]")))
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    message_rows = soup.select("[message_rows_selector]")

    # 2. Parse the time of the latest message
    latest_row = message_rows[0]
    time_element = latest_row.select_one("[time_selector]")
    time_text = time_element.get_text(strip=True)

    # 3. If active, package and return the result
    if time_text and is_within_last_hour(time_text):
        sms_content = latest_row.select_one("[content_selector]").get_text(strip=True)
        all_smss = [row.select_one("[content_selector]").get_text(strip=True) for row in message_rows]
        
        # 4. Must return a dictionary in this standard format
        return {'number': number_info['number'], 'url': number_info['url'], 'last_sms': sms_content, 'smss': all_smss}
    # ... (omitting remaining logic)
```

### **Step 4: Integrating into the Core**

1.  Open `scraper_core.py`.
2.  Find the `scrape_all_sites` function.
3.  In the `for url in target_urls:` loop, add an `elif` block to handle your new site.

    ```python
    elif "[sitename]" in url: # Use the unique domain name of the new site
        try:
            numbers = [sitename]_find_active_numbers(CHROME_SERVICE, base_url=url, user_agent=user_agent)
            if numbers:
                for number in numbers:
                    # !!! Important: Tag your results with a source label
                    number['source'] = '[SiteName]' # e.g., 'MyNewSite'
                all_results.extend(numbers)
        except Exception as e:
            print(f"[!] An error occurred while processing {url}: {e}")
    ```

### **Step 5: Debugging and Testing**

1.  **Debugging**: If you encounter parsing difficulties or a `UnicodeEncodeError`, use this trick to write the page source to a file for analysis:
    ```python
    with open("debug.html", "w", encoding="utf-8") as f:
        f.write(driver.page_source)
    ```
2.  **Add a Test**: In `tests/test_scraper_core.py`, add a smoke test for your new scraper to ensure it runs stably. You can copy the structure of `test_tempnumber_scraper_smoke_test` and replace it with your function.

---

## **‚ùì Questions?**

If you have any questions about the contribution process or any features, feel free to ask in the [Issue tracker](https://github.com/LayorX/Temporary-SMS-Receiver-Monitor/issues).

**Happy Contributing!**